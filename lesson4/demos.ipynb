{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demos\n",
    "\n",
    "This is the notebook containing the demos for Feature Store, Model Monitor, and Clarify. Testing for these exercises was performed using __2 vCPU + 4 GiB notebook instance with Python 3 (TensorFlow 2.1 Python 3.6 CPU Optimized) kernel__.\n",
    "\n",
    "## Staging\n",
    "\n",
    "We'll begin by initializing some variables that are used throughout the demos. These are often assumed to be present in code samples you'll find in the AWS documenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "bucket = session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Store\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature store is a special database to give ML systems a consistent data flow across training and inference workloads. It can ingest data in batches (for training) as well as serve input features to models with very low latency for real-time prediction.\n",
    "\n",
    "For this demo we'll use the boston housing dataset, which you can learn more about here: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 05:42:27.536011: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/09/25 05:42:34] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> generated new fontManager                                         <a href=\"file:///opt/conda/lib/python3.11/site-packages/matplotlib/font_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">font_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/matplotlib/font_manager.py#1639\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1639</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/09/25 05:42:34]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m generated new fontManager                                         \u001b]8;id=284314;file:///opt/conda/lib/python3.11/site-packages/matplotlib/font_manager.py\u001b\\\u001b[2mfont_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=708833;file:///opt/conda/lib/python3.11/site-packages/matplotlib/font_manager.py#1639\u001b\\\u001b[2m1639\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "\u001b[1m57026/57026\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data(test_split=0.1, seed=1234)\n",
    "\n",
    "# Manually add headers\n",
    "train_headers = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"]\n",
    "test_headers = [\"MEDV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-east-1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.session.Session at 0x7f77d6a3ee50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::620425758198:role/service-role/AmazonSageMaker-ExecutionRole-20250307T190825'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-620425758198'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "\n",
    "boston_train = pd.DataFrame(x_train, columns=train_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our data, we can create a feature group. Remember to attach event time and ID columns - Feature Store needs them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FeatureDefinition(feature_name='CRIM', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='ZN', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='INDUS', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='CHAS', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='NOX', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='RM', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='AGE', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='DIS', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='RAD', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='TAX', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='PTRATIO', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='B', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='LSTAT', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='EventTime', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>, collection_type=None),\n",
       " FeatureDefinition(feature_name='id', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>, collection_type=None)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_train[\"EventTime\"] = time.time()\n",
    "boston_train[\"id\"] = range(len(boston_train))\n",
    "\n",
    "# Create feature group\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "feature_group = FeatureGroup(\n",
    "    name=\"boston-features\", sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Load Feature definitions\n",
    "feature_group.load_feature_definitions(data_frame=boston_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature group is not created until we call the `create` method, let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:620425758198:feature-group/boston-features',\n",
       " 'ResponseMetadata': {'RequestId': 'fb6ac9d9-ee90-49a3-bd30-f8268626a04e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'fb6ac9d9-ee90-49a3-bd30-f8268626a04e',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '92',\n",
       "   'date': 'Sun, 09 Mar 2025 05:48:07 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group.create(\n",
    "    s3_uri=f\"s3://{bucket}/features\",\n",
    "    record_identifier_name='id',\n",
    "    event_time_feature_name=\"EventTime\",\n",
    "    role_arn=role,\n",
    "    enable_online_store=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For applications, we can create a lightweight client to retrieve data with low latency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = session.boto_session.client(\n",
    "  'sagemaker-featurestore-runtime',\n",
    "  region_name=region\n",
    ")\n",
    "\n",
    "data = runtime.get_record(\n",
    "    FeatureGroupName=\"boston-features\",\n",
    "    RecordIdentifierValueAsString=\"0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to get records before we ingest any data, the response comes back empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '5238b526-980b-4567-b240-20b595e427f4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '5238b526-980b-4567-b240-20b595e427f4',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '32',\n",
       "   'date': 'Sun, 09 Mar 2025 05:49:03 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IngestionManagerPandas(feature_group_name='boston-features', feature_definitions={'CRIM': {'FeatureName': 'CRIM', 'FeatureType': 'Fractional'}, 'ZN': {'FeatureName': 'ZN', 'FeatureType': 'Fractional'}, 'INDUS': {'FeatureName': 'INDUS', 'FeatureType': 'Fractional'}, 'CHAS': {'FeatureName': 'CHAS', 'FeatureType': 'Fractional'}, 'NOX': {'FeatureName': 'NOX', 'FeatureType': 'Fractional'}, 'RM': {'FeatureName': 'RM', 'FeatureType': 'Fractional'}, 'AGE': {'FeatureName': 'AGE', 'FeatureType': 'Fractional'}, 'DIS': {'FeatureName': 'DIS', 'FeatureType': 'Fractional'}, 'RAD': {'FeatureName': 'RAD', 'FeatureType': 'Fractional'}, 'TAX': {'FeatureName': 'TAX', 'FeatureType': 'Fractional'}, 'PTRATIO': {'FeatureName': 'PTRATIO', 'FeatureType': 'Fractional'}, 'B': {'FeatureName': 'B', 'FeatureType': 'Fractional'}, 'LSTAT': {'FeatureName': 'LSTAT', 'FeatureType': 'Fractional'}, 'EventTime': {'FeatureName': 'EventTime', 'FeatureType': 'Fractional'}, 'id': {'FeatureName': 'id', 'FeatureType': 'Integral'}}, sagemaker_fs_runtime_client_config=<botocore.config.Config object at 0x7f77d23f5f90>, sagemaker_session=<sagemaker.session.Session object at 0x7f77d6a3ee50>, max_workers=3, max_processes=1, profile_name=None, _async_result=<multiprocess.pool.MapResult object at 0x7f77a942fc90>, _processing_pool=<pool ProcessPool(ncpus=1)>, _failed_indices=[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group.ingest(data_frame=boston_train, max_workers=3, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '02c3c465-106b-41bb-8d36-ec0d84a5a05a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '02c3c465-106b-41bb-8d36-ec0d84a5a05a',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '1110',\n",
       "   'date': 'Sun, 09 Mar 2025 05:49:41 GMT'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Record': [{'FeatureName': 'CRIM', 'ValueAsString': '0.01951'},\n",
       "  {'FeatureName': 'ZN', 'ValueAsString': '17.5'},\n",
       "  {'FeatureName': 'INDUS', 'ValueAsString': '1.38'},\n",
       "  {'FeatureName': 'CHAS', 'ValueAsString': '0.0'},\n",
       "  {'FeatureName': 'NOX', 'ValueAsString': '0.4161'},\n",
       "  {'FeatureName': 'RM', 'ValueAsString': '7.104'},\n",
       "  {'FeatureName': 'AGE', 'ValueAsString': '59.5'},\n",
       "  {'FeatureName': 'DIS', 'ValueAsString': '9.2229'},\n",
       "  {'FeatureName': 'RAD', 'ValueAsString': '3.0'},\n",
       "  {'FeatureName': 'TAX', 'ValueAsString': '216.0'},\n",
       "  {'FeatureName': 'PTRATIO', 'ValueAsString': '18.6'},\n",
       "  {'FeatureName': 'B', 'ValueAsString': '393.24'},\n",
       "  {'FeatureName': 'LSTAT', 'ValueAsString': '8.05'},\n",
       "  {'FeatureName': 'EventTime', 'ValueAsString': '1741499171.4967723'},\n",
       "  {'FeatureName': 'id', 'ValueAsString': '0'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = runtime.get_record(\n",
    "    FeatureGroupName=\"boston-features\",\n",
    "    RecordIdentifierValueAsString=\"0\"\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo we create a monitoring schedule for a deployed model. We'll begin by reloading our data from the previous demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MEDV</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.01951</td>\n",
       "      <td>17.5</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4161</td>\n",
       "      <td>7.104</td>\n",
       "      <td>59.5</td>\n",
       "      <td>9.2229</td>\n",
       "      <td>3.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>393.24</td>\n",
       "      <td>8.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.5</td>\n",
       "      <td>0.14866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>6.727</td>\n",
       "      <td>79.9</td>\n",
       "      <td>2.7778</td>\n",
       "      <td>5.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>20.9</td>\n",
       "      <td>394.76</td>\n",
       "      <td>9.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.6</td>\n",
       "      <td>25.04610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6930</td>\n",
       "      <td>5.987</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.5888</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>26.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.2</td>\n",
       "      <td>3.67367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5830</td>\n",
       "      <td>6.312</td>\n",
       "      <td>51.9</td>\n",
       "      <td>3.9917</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.62</td>\n",
       "      <td>10.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.9</td>\n",
       "      <td>9.51363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>6.728</td>\n",
       "      <td>94.1</td>\n",
       "      <td>2.4961</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>6.68</td>\n",
       "      <td>18.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>17.9</td>\n",
       "      <td>18.81100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5970</td>\n",
       "      <td>4.628</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.5539</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>28.79</td>\n",
       "      <td>34.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>14.5</td>\n",
       "      <td>8.49213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5840</td>\n",
       "      <td>6.348</td>\n",
       "      <td>86.1</td>\n",
       "      <td>2.0527</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>83.45</td>\n",
       "      <td>17.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>12.7</td>\n",
       "      <td>4.66883</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>5.976</td>\n",
       "      <td>87.9</td>\n",
       "      <td>2.5806</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>10.48</td>\n",
       "      <td>19.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>17.8</td>\n",
       "      <td>0.31827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5440</td>\n",
       "      <td>5.914</td>\n",
       "      <td>83.2</td>\n",
       "      <td>3.9986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>390.70</td>\n",
       "      <td>18.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>23.7</td>\n",
       "      <td>0.12757</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4280</td>\n",
       "      <td>6.393</td>\n",
       "      <td>7.8</td>\n",
       "      <td>7.0355</td>\n",
       "      <td>6.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>374.71</td>\n",
       "      <td>5.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     MEDV      CRIM    ZN  INDUS  CHAS     NOX     RM    AGE     DIS   RAD  \\\n",
       "0    33.0   0.01951  17.5   1.38   0.0  0.4161  7.104   59.5  9.2229   3.0   \n",
       "1    27.5   0.14866   0.0   8.56   0.0  0.5200  6.727   79.9  2.7778   5.0   \n",
       "2     5.6  25.04610   0.0  18.10   0.0  0.6930  5.987  100.0  1.5888  24.0   \n",
       "3    21.2   3.67367   0.0  18.10   0.0  0.5830  6.312   51.9  3.9917  24.0   \n",
       "4    14.9   9.51363   0.0  18.10   0.0  0.7130  6.728   94.1  2.4961  24.0   \n",
       "..    ...       ...   ...    ...   ...     ...    ...    ...     ...   ...   \n",
       "450  17.9  18.81100   0.0  18.10   0.0  0.5970  4.628  100.0  1.5539  24.0   \n",
       "451  14.5   8.49213   0.0  18.10   0.0  0.5840  6.348   86.1  2.0527  24.0   \n",
       "452  12.7   4.66883   0.0  18.10   0.0  0.7130  5.976   87.9  2.5806  24.0   \n",
       "453  17.8   0.31827   0.0   9.90   0.0  0.5440  5.914   83.2  3.9986   4.0   \n",
       "454  23.7   0.12757  30.0   4.93   0.0  0.4280  6.393    7.8  7.0355   6.0   \n",
       "\n",
       "       TAX  PTRATIO       B  LSTAT  \n",
       "0    216.0     18.6  393.24   8.05  \n",
       "1    384.0     20.9  394.76   9.42  \n",
       "2    666.0     20.2  396.90  26.77  \n",
       "3    666.0     20.2  388.62  10.58  \n",
       "4    666.0     20.2    6.68  18.71  \n",
       "..     ...      ...     ...    ...  \n",
       "450  666.0     20.2   28.79  34.37  \n",
       "451  666.0     20.2   83.45  17.64  \n",
       "452  666.0     20.2   10.48  19.01  \n",
       "453  304.0     18.4  390.70  18.33  \n",
       "454  300.0     16.6  374.71   5.19  \n",
       "\n",
       "[455 rows x 14 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "import pandas as pd\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data(test_split=0.1, seed=1234)\n",
    "headers = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"]\n",
    "\n",
    "\n",
    "train = pd.DataFrame(x_train, columns=headers)\n",
    "train[\"MEDV\"] = y_train\n",
    "\n",
    "# Target variable must come first per https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html\n",
    "train.set_index(train.pop('MEDV'), inplace=True)\n",
    "train.reset_index(inplace=True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  pd.DataFrame(x_test)\n",
    "test[\"MEDV\"] = y_test\n",
    "\n",
    "# Target variable must come first per https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html\n",
    "test.set_index(test.pop('MEDV'), inplace=True)\n",
    "test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll upload the data to S3 as train and validation data, then train a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train.csv\", header=False, index=False)\n",
    "test.to_csv(\"validation.csv\", header=False, index=False)\n",
    "\n",
    "val_location = session.upload_data('./validation.csv', key_prefix=\"data\")\n",
    "train_location = session.upload_data('./train.csv', key_prefix=\"data\")\n",
    "\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data=val_location, content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/09/25 05:51:00] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Ignoring unnecessary instance type: <span style=\"color: #e100e1; text-decoration-color: #e100e1; font-style: italic\">None</span>.                            <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#530\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">530</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/09/25 05:51:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Ignoring unnecessary instance type: \u001b[3;38;2;225;0;225mNone\u001b[0m.                            \u001b]8;id=475060;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=571750;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#530\u001b\\\u001b[2m530\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/09/25 05:51:01] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> SageMaker Python SDK will collect telemetry to help us better  <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/telemetry/telemetry_logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">telemetry_logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/telemetry/telemetry_logging.py#91\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">91</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         understand our user's needs, diagnose issues, and deliver      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         additional features.                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To opt out of telemetry, please disable via TelemetryOptOut    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         parameter in SDK defaults config. For more information, refer  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         to                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://sagemaker.readthedocs.io/en/stable/overview.html#confi</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">guring-and-using-defaults-with-the-sagemaker-python-sdk.</span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/09/25 05:51:01]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m SageMaker Python SDK will collect telemetry to help us better  \u001b]8;id=920169;file:///opt/conda/lib/python3.11/site-packages/sagemaker/telemetry/telemetry_logging.py\u001b\\\u001b[2mtelemetry_logging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=401082;file:///opt/conda/lib/python3.11/site-packages/sagemaker/telemetry/telemetry_logging.py#91\u001b\\\u001b[2m91\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         understand our user's needs, diagnose issues, and deliver      \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         additional features.                                           \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To opt out of telemetry, please disable via TelemetryOptOut    \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         parameter in SDK defaults config. For more information, refer  \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         to                                                             \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mhttps://sagemaker.readthedocs.io/en/stable/overview.html#confi\u001b[0m \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mguring-and-using-defaults-with-the-sagemaker-python-sdk.\u001b[0m       \u001b[2m                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating training-job with name: xgboost-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-09-05-51-01-627       <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#1042\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1042</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating training-job with name: xgboost-\u001b[1;36m2025\u001b[0m-03-09-05-51-01-627       \u001b]8;id=372536;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=636498;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#1042\u001b\\\u001b[2m1042\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-09 05:51:02 Starting - Starting the training job...\n",
      "..25-03-09 05:51:26 Starting - Preparing the instances for training.\n",
      ".....03-09 05:52:02 Downloading - Downloading input data.\n",
      "....\u001b[34mArguments: train\u001b[0mng - Downloading the training image.\n",
      "\u001b[34m[2025-03-09:05:53:49:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2025-03-09:05:53:49:INFO] File size need to be processed in the node: 0.04mb. Available memory size in the node: 8565.78mb\u001b[0m\n",
      "\u001b[34m[2025-03-09:05:53:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[05:53:49] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[05:53:49] 455x13 matrix with 5915 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2025-03-09:05:53:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[05:53:49] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[05:53:49] 51x13 matrix with 663 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:19.0908#011validation-rmse:22.4172\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'validation-rmse' will be used for early stopping.\u001b[0m\n",
      "\u001b[34mWill train until validation-rmse hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:15.5062#011validation-rmse:18.8233\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:12.6666#011validation-rmse:16.1071\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:10.3815#011validation-rmse:13.9311\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:8.58252#011validation-rmse:12.2425\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:7.11635#011validation-rmse:10.8612\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:5.93565#011validation-rmse:9.83747\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:5.03626#011validation-rmse:9.10031\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:4.29516#011validation-rmse:8.5271\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:3.72553#011validation-rmse:8.07524\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-rmse:3.25219#011validation-rmse:7.77361\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-rmse:2.88655#011validation-rmse:7.40263\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-rmse:2.62248#011validation-rmse:7.16337\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-rmse:2.42243#011validation-rmse:6.98138\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-rmse:2.27546#011validation-rmse:6.89823\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-rmse:2.15968#011validation-rmse:6.81207\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-rmse:2.05751#011validation-rmse:6.67012\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-rmse:1.9714#011validation-rmse:6.50705\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-rmse:1.92249#011validation-rmse:6.49466\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-rmse:1.85751#011validation-rmse:6.38751\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-rmse:1.80983#011validation-rmse:6.34459\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-rmse:1.75717#011validation-rmse:6.2741\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-rmse:1.72296#011validation-rmse:6.27353\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-rmse:1.68481#011validation-rmse:6.21085\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-rmse:1.65855#011validation-rmse:6.22027\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-rmse:1.64414#011validation-rmse:6.21012\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-rmse:1.61342#011validation-rmse:6.16552\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-rmse:1.5856#011validation-rmse:6.09461\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-rmse:1.54871#011validation-rmse:6.07782\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-rmse:1.51933#011validation-rmse:6.08499\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-rmse:1.49798#011validation-rmse:6.08019\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-rmse:1.48293#011validation-rmse:6.08741\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-rmse:1.44964#011validation-rmse:6.07008\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-rmse:1.40987#011validation-rmse:6.00792\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-rmse:1.38824#011validation-rmse:5.96544\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-rmse:1.36156#011validation-rmse:5.96218\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-rmse:1.3333#011validation-rmse:5.90715\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-rmse:1.31574#011validation-rmse:5.88819\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-rmse:1.27964#011validation-rmse:5.894\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-rmse:1.24837#011validation-rmse:5.93158\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-rmse:1.22705#011validation-rmse:5.94961\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-rmse:1.19355#011validation-rmse:5.91585\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-rmse:1.18221#011validation-rmse:5.89457\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-rmse:1.15167#011validation-rmse:5.88793\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-rmse:1.14491#011validation-rmse:5.85042\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-rmse:1.13425#011validation-rmse:5.82337\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-rmse:1.11415#011validation-rmse:5.80667\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-rmse:1.09061#011validation-rmse:5.80032\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[48]#011train-rmse:1.06944#011validation-rmse:5.83868\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-rmse:1.04835#011validation-rmse:5.79362\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[50]#011train-rmse:1.03953#011validation-rmse:5.76721\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[51]#011train-rmse:1.02674#011validation-rmse:5.7244\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[52]#011train-rmse:1.00705#011validation-rmse:5.72429\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[53]#011train-rmse:0.9937#011validation-rmse:5.68678\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[54]#011train-rmse:0.973848#011validation-rmse:5.64683\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[55]#011train-rmse:0.966061#011validation-rmse:5.64699\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[56]#011train-rmse:0.953159#011validation-rmse:5.63833\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 12 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[57]#011train-rmse:0.946136#011validation-rmse:5.63045\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 20 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[58]#011train-rmse:0.93815#011validation-rmse:5.6111\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[59]#011train-rmse:0.923381#011validation-rmse:5.58822\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 24 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[60]#011train-rmse:0.914201#011validation-rmse:5.56656\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[61]#011train-rmse:0.902582#011validation-rmse:5.5652\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[62]#011train-rmse:0.893344#011validation-rmse:5.53575\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[63]#011train-rmse:0.884696#011validation-rmse:5.52879\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 30 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[64]#011train-rmse:0.884728#011validation-rmse:5.52727\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 24 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[65]#011train-rmse:0.884702#011validation-rmse:5.52773\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 16 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[66]#011train-rmse:0.879611#011validation-rmse:5.51016\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[67]#011train-rmse:0.875472#011validation-rmse:5.52188\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 24 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[68]#011train-rmse:0.870288#011validation-rmse:5.51781\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 10 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[69]#011train-rmse:0.869059#011validation-rmse:5.54826\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[70]#011train-rmse:0.858869#011validation-rmse:5.54909\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 18 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[71]#011train-rmse:0.858814#011validation-rmse:5.54975\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 18 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[72]#011train-rmse:0.858791#011validation-rmse:5.55039\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[73]#011train-rmse:0.852737#011validation-rmse:5.53916\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 14 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[74]#011train-rmse:0.852761#011validation-rmse:5.53877\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 24 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[75]#011train-rmse:0.850308#011validation-rmse:5.54095\u001b[0m\n",
      "\u001b[34m[05:53:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[76]#011train-rmse:0.845425#011validation-rmse:5.5454\u001b[0m\n",
      "\u001b[34mStopping. Best iteration:\u001b[0m\n",
      "\u001b[34m[66]#011train-rmse:0.879611#011validation-rmse:5.51016\u001b[0m\n",
      "\n",
      "2025-03-09 05:54:07 Training - Training image download completed. Training in progress.\n",
      "2025-03-09 05:54:07 Uploading - Uploading generated training model\n",
      "2025-03-09 05:54:07 Completed - Training job completed\n",
      "Training seconds: 125\n",
      "Billable seconds: 125\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "algo_image = sagemaker.image_uris.retrieve(\"xgboost\", region, version='latest')\n",
    "s3_output_location = f\"s3://{bucket}/models/boston_model\"\n",
    "\n",
    "model=sagemaker.estimator.Estimator(\n",
    "    image_uri=algo_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    volume_size=5,\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")\n",
    "\n",
    "model.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        objective='reg:linear',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=200)\n",
    "\n",
    "\n",
    "model.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the training job has finished, we can configure a deployment for data capture, then deploy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_uri = f's3://{bucket}/data-capture'\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=capture_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/09/25 05:54:19] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating model with name: xgboost-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-09-05-54-19-132              <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4094\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4094</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/09/25 05:54:19]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating model with name: xgboost-\u001b[1;36m2025\u001b[0m-03-09-05-54-19-132              \u001b]8;id=593957;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=292254;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4094\u001b\\\u001b[2m4094\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint-config with name xgboost-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-09-05-54-19-132     <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#5889\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5889</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint-config with name xgboost-\u001b[1;36m2025\u001b[0m-03-09-05-54-19-132     \u001b]8;id=920904;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=685031;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#5889\u001b\\\u001b[2m5889\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/09/25 05:54:20] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint with name xgboost-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-09-05-54-19-132            <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4711\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4711</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/09/25 05:54:20]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint with name xgboost-\u001b[1;36m2025\u001b[0m-03-09-05-54-19-132            \u001b]8;id=687339;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=334397;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#4711\u001b\\\u001b[2m4711\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = model.deploy(\n",
    "    initial_instance_count=1, instance_type='ml.m4.xlarge',\n",
    "    data_capture_config=data_capture_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can provide some sample code to test the deployed model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.serializer = sagemaker.serializers.CSVSerializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = test.copy()\n",
    "inputs = inputs.drop(columns=inputs.columns[0])\n",
    "\n",
    "x_pred = xgb_predictor.predict(inputs.sample(5).values).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.inputs.TrainingInput at 0x7f77a941f3d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_input_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the Model Monitor and suggest a baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/09/25 05:59:26] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Ignoring unnecessary instance type: <span style=\"color: #e100e1; text-decoration-color: #e100e1; font-style: italic\">None</span>.                            <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#530\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">530</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/09/25 05:59:26]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Ignoring unnecessary instance type: \u001b[3;38;2;225;0;225mNone\u001b[0m.                            \u001b]8;id=647265;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=361752;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#530\u001b\\\u001b[2m530\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/09/25 05:59:30] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating processing-job with name                                      <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#1575\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1575</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         baseline-suggestion-job-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-09-05-59-30-482                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/09/25 05:59:30]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating processing-job with name                                      \u001b]8;id=268656;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=76590;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#1575\u001b\\\u001b[2m1575\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         baseline-suggestion-job-\u001b[1;36m2025\u001b[0m-03-09-05-59-30-482                        \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........\u001b[34m2025-03-09 06:01:22.115464: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:22.115496: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:23.782304: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:23.782335: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:23.782360: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-2-231-158.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:23.782635: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:25,411 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:620425758198:processing-job/baseline-suggestion-job-2025-03-09-05-59-30-482', 'ProcessingJobName': 'baseline-suggestion-job-2025-03-09-05-59-30-482', 'Environment': {'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-620425758198/data/train.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-620425758198/model-monitor/baselining/baseline-suggestion-job-2025-03-09-05-59-30-482/results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::620425758198:role/service-role/AmazonSageMaker-ExecutionRole-20250307T190825', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:25,411 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:25,411 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:25,412 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:25,412 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:25,412 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:25,473 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:25,473 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:25,474 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0'}\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:25,482 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:25,483 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:25,483 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:25,944 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.231.158\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timel\u001b[0m\n",
      "\u001b[34mine-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_392\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:25,955 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:25,959 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-7c456261-2d41-470b-8087-17cc6c526a41\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,498 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,509 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,510 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,512 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,517 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,517 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,517 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,517 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,549 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,563 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,563 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,568 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,571 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Mar 09 06:01:26\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,573 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,573 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,574 INFO util.GSet: 2.0% max memory 3.1 GB = 63.5 MB\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,575 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,610 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,614 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,614 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,614 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,614 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,614 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,614 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,614 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,614 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,614 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,615 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,615 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,641 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,641 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,641 INFO util.GSet: 1.0% max memory 3.1 GB = 31.7 MB\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,641 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,643 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,643 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,643 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,643 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,647 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,651 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,651 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,651 INFO util.GSet: 0.25% max memory 3.1 GB = 7.9 MB\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,651 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,688 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,688 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,688 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,691 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,692 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,693 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,693 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,693 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 974.9 KB\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,693 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,715 INFO namenode.FSImage: Allocated new BlockPoolId: BP-2122233930-10.2.231.158-1741500086708\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,728 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,736 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,820 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,832 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,836 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.2.231.158\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:26,846 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:28,906 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:28,906 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:30,984 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:30,985 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:33,072 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:33,073 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:35,165 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:35,166 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:37,284 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:37,285 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:47,294 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:48,930 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:49,397 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:49,439 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:49,449 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,028 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,051 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,051 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,052 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,052 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,077 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11507, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,090 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,092 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,143 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,143 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,143 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,144 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,144 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,550 INFO util.Utils: Successfully started service 'sparkDriver' on port 43023.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,578 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,611 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,631 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,631 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,667 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,687 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-315b6169-af75-45b4-be97-cfbb0d618317\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,705 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,744 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:50,779 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.2.231.158:43023/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1741500110025\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:51,250 INFO client.RMProxy: Connecting to ResourceManager at /10.2.231.158:8032\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:51,973 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:51,974 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:51,981 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15692 MB per container)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:51,981 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:51,981 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:51,982 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:51,988 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:52,073 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:54,353 INFO yarn.Client: Uploading resource file:/tmp/spark-d69b3e10-27e1-4b58-a959-aeb395fb3004/__spark_libs__6375599727687557283.zip -> hdfs://10.2.231.158/user/root/.sparkStaging/application_1741500092616_0001/__spark_libs__6375599727687557283.zip\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:56,341 INFO yarn.Client: Uploading resource file:/tmp/spark-d69b3e10-27e1-4b58-a959-aeb395fb3004/__spark_conf__9019635906324587268.zip -> hdfs://10.2.231.158/user/root/.sparkStaging/application_1741500092616_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:56,784 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:56,784 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:56,784 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:56,784 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:56,785 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:56,816 INFO yarn.Client: Submitting application application_1741500092616_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:57,015 INFO impl.YarnClientImpl: Submitted application application_1741500092616_0001\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:58,020 INFO yarn.Client: Application report for application_1741500092616_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:58,024 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Sun Mar 09 06:01:57 +0000 2025] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1741500116915\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1741500092616_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-03-09 06:01:59,028 INFO yarn.Client: Application report for application_1741500092616_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:00,031 INFO yarn.Client: Application report for application_1741500092616_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:01,034 INFO yarn.Client: Application report for application_1741500092616_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:02,038 INFO yarn.Client: Application report for application_1741500092616_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:02,358 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1741500092616_0001), /proxy/application_1741500092616_0001\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:03,043 INFO yarn.Client: Application report for application_1741500092616_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:03,045 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.2.231.158\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1741500116915\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1741500092616_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:03,048 INFO cluster.YarnClientSchedulerBackend: Application application_1741500092616_0001 has started running.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:03,063 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33075.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:03,064 INFO netty.NettyBlockTransferService: Server created on 10.2.231.158:33075\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:03,066 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:03,076 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.2.231.158, 33075, None)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:03,080 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.2.231.158:33075 with 1458.6 MiB RAM, BlockManagerId(driver, 10.2.231.158, 33075, None)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:03,093 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.2.231.158, 33075, None)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:03,094 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.2.231.158, 33075, None)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:03,277 INFO util.log: Logging initialized @15755ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:03,877 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:08,366 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.231.158:54598) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:08,550 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:36893 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 36893, None)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:21,145 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:21,341 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:21,397 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:21,403 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:22,417 INFO datasources.InMemoryFileIndex: It took 39 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:22,578 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:22,894 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:22,897 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.231.158:33075 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:22,903 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:23,312 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:23,315 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:23,319 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 35152\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:23,432 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:23,452 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:23,452 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:23,453 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:23,454 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:23,462 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:23,521 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:23,524 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:23,525 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.2.231.158:33075 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:23,525 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:23,540 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:23,541 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:23,580 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4618 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:23,802 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:36893 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:24,592 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:36893 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:24,888 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1323 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:24,890 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:24,895 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.400 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:24,899 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:24,900 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:24,901 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.468149 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:25,075 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.2.231.158:33075 in memory (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:25,080 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:36893 in memory (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:25,102 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:36893 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:25,109 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.2.231.158:33075 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,121 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,122 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,125 INFO datasources.FileSourceStrategy: Output Data Schema: struct<_c0: string, _c1: string, _c2: string, _c3: string, _c4: string ... 12 more fields>\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,321 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,337 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,338 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.2.231.158:33075 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,339 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,353 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,389 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,391 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,391 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,391 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,394 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,396 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,457 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.4 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,460 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,461 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.2.231.158:33075 (size: 8.1 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,461 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,462 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,462 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,465 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:27,522 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:36893 (size: 8.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:28,389 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:36893 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:28,515 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:36893 (size: 38.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:28,629 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1166 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:28,629 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:28,631 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 1.231 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:28,631 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:28,631 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:28,635 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 1.245967 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:28,933 INFO codegen.CodeGenerator: Code generated in 220.465822 ms\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:29,567 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:29,720 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:29,725 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:29,725 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:29,725 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:29,728 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:29,729 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:29,753 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 114.3 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:29,788 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:29,790 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.2.231.158:33075 (size: 34.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:29,791 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:29,794 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:29,794 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:29,811 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:29,813 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.2.231.158:33075 in memory (size: 8.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:29,822 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:36893 in memory (size: 8.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:29,845 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:36893 (size: 34.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,013 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1203 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,013 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,015 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.282 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,015 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,016 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,016 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,016 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,084 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,087 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,087 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,087 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,087 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,088 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,100 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 166.8 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,102 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 45.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,103 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.2.231.158:33075 (size: 45.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,104 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,105 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,105 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,107 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,127 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:36893 (size: 45.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,175 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.2.231.158:54598\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,599 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 493 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,600 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,601 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.507 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,601 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,602 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,604 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.519104 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:31,646 INFO codegen.CodeGenerator: Code generated in 32.301013 ms\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,002 INFO codegen.CodeGenerator: Code generated in 55.16761 ms\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,081 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,083 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,083 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,084 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,084 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,086 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,131 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 38.2 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,135 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,136 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.2.231.158:33075 (size: 16.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,140 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,140 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,141 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,143 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,162 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:36893 (size: 16.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,484 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 341 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,485 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,485 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.394 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,487 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,488 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,489 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 0.407099 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,953 INFO codegen.CodeGenerator: Code generated in 105.30527 ms\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,961 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,962 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,962 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,962 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,963 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,964 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,971 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 73.6 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,972 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.4 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,973 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.2.231.158:33075 (size: 23.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,974 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,974 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,974 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,975 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:32,998 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:36893 (size: 23.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,092 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 117 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,092 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,093 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.127 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,093 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,093 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,094 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,094 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,290 INFO codegen.CodeGenerator: Code generated in 94.02728 ms\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,303 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,305 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,305 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,305 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,305 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,306 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,310 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,313 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,314 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.2.231.158:33075 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,315 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,315 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,315 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,317 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,332 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:36893 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,340 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.2.231.158:54598\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,419 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 103 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,420 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,421 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.114 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,421 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,421 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,425 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.122033 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,531 INFO codegen.CodeGenerator: Code generated in 66.509021 ms\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,634 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,638 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,638 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,639 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,639 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,639 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,641 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,652 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 30.5 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,654 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 13.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,655 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.2.231.158:33075 (size: 13.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,656 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,656 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,656 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,658 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:33,678 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:36893 (size: 13.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:34,923 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1265 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:34,924 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:34,926 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 1.284 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:34,928 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:34,929 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:34,929 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:34,929 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:34,930 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:34,933 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:34,939 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:34,940 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.2.231.158:33075 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:34,949 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:34,951 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:34,951 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:34,954 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:34,972 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:36893 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:34,979 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.2.231.158:54598\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,041 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 89 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,042 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,043 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.111 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,043 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,043 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,043 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 1.409099 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,263 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,263 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,263 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,264 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,264 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,267 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,273 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 83.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,276 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 27.2 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,277 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.2.231.158:33075 (size: 27.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,277 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,278 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,284 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,286 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,301 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:36893 (size: 27.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,477 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 191 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,477 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,478 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.210 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,478 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,479 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,479 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,479 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,523 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,524 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,524 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,524 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,524 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,527 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,533 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 167.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,535 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 46.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,535 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.2.231.158:33075 (size: 46.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,536 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,536 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,536 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,537 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,549 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:36893 (size: 46.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,558 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.2.231.158:54598\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,642 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 105 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,642 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,643 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.115 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,643 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,643 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,643 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.120490 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,779 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,780 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,780 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,781 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,781 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,782 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,789 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 38.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,791 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,791 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.2.231.158:33075 (size: 16.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,792 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,792 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,792 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,794 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,810 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:36893 (size: 16.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,834 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 40 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,837 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,838 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.055 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,839 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,839 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,839 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.059628 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,979 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,979 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,980 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,980 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,980 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,980 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,986 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 73.6 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,988 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.4 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,988 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.2.231.158:33075 (size: 23.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,989 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,989 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,989 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:35,991 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,006 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:36893 (size: 23.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,026 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 36 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,026 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,027 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.043 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,027 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,027 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,027 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,027 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,108 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,119 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:36893 in memory (size: 13.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,121 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,121 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,121 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,122 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,121 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.2.231.158:33075 in memory (size: 13.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,122 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,127 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 66.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,129 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,130 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.2.231.158:33075 (size: 19.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,130 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,131 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,131 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,133 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,154 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:36893 (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,159 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.2.231.158:54598\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,166 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 33 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,167 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,168 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.043 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,169 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,169 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,170 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.060518 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,195 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:36893 in memory (size: 34.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,202 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.2.231.158:33075 in memory (size: 34.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,236 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,237 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,239 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,239 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,239 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,239 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,241 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,251 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 30.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,253 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,254 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.2.231.158:33075 (size: 14.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,254 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,255 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,255 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,257 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:36893 in memory (size: 19.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,258 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,263 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.2.231.158:33075 in memory (size: 19.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,271 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:36893 (size: 14.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,274 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.2.231.158:33075 in memory (size: 45.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,287 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:36893 in memory (size: 45.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,305 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 49 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,305 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,306 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.064 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,306 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,307 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,307 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,307 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,308 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,310 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,312 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,313 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.2.231.158:33075 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,313 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,314 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,314 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,315 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:36893 in memory (size: 23.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,316 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,317 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.2.231.158:33075 in memory (size: 23.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,334 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.2.231.158:33075 in memory (size: 23.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,338 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:36893 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,338 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:36893 in memory (size: 23.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,346 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.2.231.158:54598\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,382 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.2.231.158:33075 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,394 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:36893 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,396 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 81 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,396 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,397 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.088 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,397 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,397 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,398 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.161566 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,410 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.2.231.158:33075 in memory (size: 16.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,411 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:36893 in memory (size: 16.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,421 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.2.231.158:33075 in memory (size: 46.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,422 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:36893 in memory (size: 46.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,430 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.2.231.158:33075 in memory (size: 27.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,431 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:36893 in memory (size: 27.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,436 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.2.231.158:33075 in memory (size: 16.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,439 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:36893 in memory (size: 16.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,558 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,558 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,558 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,559 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,559 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,559 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,564 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 73.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,566 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 25.8 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,566 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.2.231.158:33075 (size: 25.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,567 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,568 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,568 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,569 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,581 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:36893 (size: 25.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,808 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 239 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,808 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,809 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.249 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,809 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,809 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,809 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,809 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,847 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,848 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,849 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,849 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,849 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,850 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,856 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 141.6 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,859 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 40.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,860 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.2.231.158:33075 (size: 40.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,865 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,865 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,866 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,867 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,879 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:36893 (size: 40.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:36,887 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.2.231.158:54598\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,029 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 162 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,029 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,030 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.179 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,033 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,034 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,037 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.189504 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,052 INFO codegen.CodeGenerator: Code generated in 11.621376 ms\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,128 INFO codegen.CodeGenerator: Code generated in 18.501454 ms\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,160 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,161 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,162 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,162 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,163 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,163 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,179 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 37.2 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,182 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,182 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.2.231.158:33075 (size: 16.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,183 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,184 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,184 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,185 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,200 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:36893 (size: 16.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,286 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 101 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,286 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,294 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.130 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,294 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,294 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,295 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.134341 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,636 INFO codegen.CodeGenerator: Code generated in 123.266953 ms\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,647 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,647 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,648 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,648 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,648 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,649 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,652 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 63.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,654 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 20.8 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,655 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.2.231.158:33075 (size: 20.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,655 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,658 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,658 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,660 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,673 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:36893 (size: 20.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,761 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 100 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,761 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,761 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.111 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,761 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,761 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,761 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,761 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,846 INFO codegen.CodeGenerator: Code generated in 49.096457 ms\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,858 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,859 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,859 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,860 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,860 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,860 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,863 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 55.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,865 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,866 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.2.231.158:33075 (size: 16.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,866 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,867 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,867 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,869 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,879 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:36893 (size: 16.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,883 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.2.231.158:54598\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,939 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 71 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,939 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,941 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.078 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,942 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,945 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:37,946 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.087864 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,026 INFO codegen.CodeGenerator: Code generated in 63.654825 ms\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,144 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,155 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,156 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,156 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,157 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,157 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,161 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,167 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 30.5 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,169 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,169 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.2.231.158:33075 (size: 14.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,170 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,170 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,170 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,173 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,192 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:36893 (size: 14.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,295 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 122 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,295 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,295 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.133 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,296 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,296 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,296 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,296 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,297 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,300 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,302 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,303 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.2.231.158:33075 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,304 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,306 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,307 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,309 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,330 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:36893 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,334 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.2.231.158:54598\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,366 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 57 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,366 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,367 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.068 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,368 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,372 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,373 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.228486 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,722 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,783 INFO codegen.CodeGenerator: Code generated in 15.414994 ms\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,792 INFO scheduler.DAGScheduler: Registering RDD 121 (count at StatsGenerator.scala:66) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,793 INFO scheduler.DAGScheduler: Got map stage job 20 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,793 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,793 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,794 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,795 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,798 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 22.5 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,802 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,804 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.2.231.158:33075 (size: 10.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,805 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,805 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,805 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,807 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,825 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:36893 (size: 10.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,888 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 81 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,888 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,889 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (count at StatsGenerator.scala:66) finished in 0.093 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,889 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,889 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,889 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,889 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,932 INFO codegen.CodeGenerator: Code generated in 22.90769 ms\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,961 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,963 INFO scheduler.DAGScheduler: Got job 21 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,965 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,966 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,966 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,966 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,969 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 11.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,970 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,972 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.2.231.158:33075 (size: 5.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,972 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,973 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,973 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,975 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,985 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:36893 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:38,991 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.2.231.158:54598\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,026 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 51 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,026 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,027 INFO scheduler.DAGScheduler: ResultStage 31 (count at StatsGenerator.scala:66) finished in 0.060 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,027 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,027 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,028 INFO scheduler.DAGScheduler: Job 21 finished: count at StatsGenerator.scala:66, took 0.065873 s\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,362 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,377 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,434 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,435 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,447 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,461 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,496 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,499 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,502 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,506 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,545 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,545 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,545 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,570 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,572 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-d69b3e10-27e1-4b58-a959-aeb395fb3004\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,585 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-6846a895-2fc6-427b-bf37-ba1eb363039c\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,688 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2025-03-09 06:02:39,688 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingJob at 0x7f77a1618210>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_monitor.suggest_baseline(\n",
    "    baseline_dataset=f's3://{bucket}/data/train.csv',\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the Model Monitor must be scheduled, or it won't actually run regular processing jobs on the captured data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/09/25 06:04:55] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Skipping checksum validation. Response did not contain one of the  <a href=\"file:///opt/conda/lib/python3.11/site-packages/botocore/httpchecksum.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">httpchecksum.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/botocore/httpchecksum.py#481\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">481</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         following algorithms: <span style=\"font-weight: bold\">[</span><span style=\"color: #008700; text-decoration-color: #008700\">'crc32'</span>, <span style=\"color: #008700; text-decoration-color: #008700\">'sha1'</span>, <span style=\"color: #008700; text-decoration-color: #008700\">'sha256'</span><span style=\"font-weight: bold\">]</span>.                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/09/25 06:04:55]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Skipping checksum validation. Response did not contain one of the  \u001b]8;id=483614;file:///opt/conda/lib/python3.11/site-packages/botocore/httpchecksum.py\u001b\\\u001b[2mhttpchecksum.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=94431;file:///opt/conda/lib/python3.11/site-packages/botocore/httpchecksum.py#481\u001b\\\u001b[2m481\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         following algorithms: \u001b[1m[\u001b[0m\u001b[38;2;0;135;0m'crc32'\u001b[0m, \u001b[38;2;0;135;0m'sha1'\u001b[0m, \u001b[38;2;0;135;0m'sha256'\u001b[0m\u001b[1m]\u001b[0m.                 \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/09/25 06:04:56] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Skipping checksum validation. Response did not contain one of the  <a href=\"file:///opt/conda/lib/python3.11/site-packages/botocore/httpchecksum.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">httpchecksum.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/botocore/httpchecksum.py#481\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">481</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         following algorithms: <span style=\"font-weight: bold\">[</span><span style=\"color: #008700; text-decoration-color: #008700\">'crc32'</span>, <span style=\"color: #008700; text-decoration-color: #008700\">'sha1'</span>, <span style=\"color: #008700; text-decoration-color: #008700\">'sha256'</span><span style=\"font-weight: bold\">]</span>.                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/09/25 06:04:56]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Skipping checksum validation. Response did not contain one of the  \u001b]8;id=818243;file:///opt/conda/lib/python3.11/site-packages/botocore/httpchecksum.py\u001b\\\u001b[2mhttpchecksum.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=174866;file:///opt/conda/lib/python3.11/site-packages/botocore/httpchecksum.py#481\u001b\\\u001b[2m481\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         following algorithms: \u001b[1m[\u001b[0m\u001b[38;2;0;135;0m'crc32'\u001b[0m, \u001b[38;2;0;135;0m'sha1'\u001b[0m, \u001b[38;2;0;135;0m'sha256'\u001b[0m\u001b[1m]\u001b[0m.                 \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating Monitoring Schedule with name:                       <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/model_monitor/model_monitoring.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">model_monitoring.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/model_monitor/model_monitoring.py#1560\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1560</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         my-monitoring-schedule                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                        </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating Monitoring Schedule with name:                       \u001b]8;id=768494;file:///opt/conda/lib/python3.11/site-packages/sagemaker/model_monitor/model_monitoring.py\u001b\\\u001b[2mmodel_monitoring.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=294724;file:///opt/conda/lib/python3.11/site-packages/sagemaker/model_monitor/model_monitoring.py#1560\u001b\\\u001b[2m1560\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         my-monitoring-schedule                                        \u001b[2m                        \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "\n",
    "my_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name='my-monitoring-schedule',\n",
    "    endpoint_input=xgb_predictor.endpoint_name,\n",
    "    statistics=my_monitor.baseline_statistics(),\n",
    "    constraints=my_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clarify\n",
    "\n",
    "This Clarify demo builds on the previous demo: we follow the same pattern of define-configure-schedule for our Monitor. Clarify, however, needs more config. We define `SHAPConfig`, `ModelConfig`, `ExplainabilityAnalysisConfig`, and pass them all to the scheduling method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_explainability_monitor = sagemaker.model_monitor.ModelExplainabilityMonitor(\n",
    "    role=role,\n",
    "    sagemaker_session=session,\n",
    "    max_runtime_in_seconds=1800,\n",
    ")\n",
    "\n",
    "\n",
    "shap_config = sagemaker.clarify.SHAPConfig(\n",
    "    baseline=[train.mean().astype(int).to_list()[1:]],\n",
    "    num_samples=int(x_train.size),\n",
    "    agg_method=\"mean_abs\",\n",
    "    save_local_shap_values=False,\n",
    ")\n",
    "\n",
    "\n",
    "model_config = sagemaker.clarify.ModelConfig(\n",
    "    model_name=\"xgboost-2021-08-25-15-19-33-499\",\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    content_type=\"text/csv\",\n",
    "    accept_type=\"text/csv\",\n",
    ")\n",
    "\n",
    "analysis_config = sagemaker.model_monitor.ExplainabilityAnalysisConfig(\n",
    "        explainability_config=shap_config,\n",
    "        model_config=model_config,\n",
    "        headers=train.columns.to_list()[1:],\n",
    "    )\n",
    "\n",
    "explainability_uri = f\"s3://{bucket}/model_explainability\"\n",
    "model_explainability_monitor.create_monitoring_schedule(\n",
    "    output_s3_uri=explainability_uri,\n",
    "    analysis_config=analysis_config,\n",
    "    endpoint_input=xgb_predictor.endpoint_name,\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleting Monitoring Schedule with name: monitoring-schedule-2021-09-02-22-00-36-508\n"
     ]
    }
   ],
   "source": [
    "model_explainability_monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleting Monitoring Schedule with name: my-monitoring-schedule\n"
     ]
    }
   ],
   "source": [
    "my_monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
